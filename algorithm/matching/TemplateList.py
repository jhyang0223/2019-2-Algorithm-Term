
templatebyProf = [
"DEBUG org.apache.hadoop.hdfs.DFSClient: computePacketChunkSize: src=.*, chunkSize=.*, chunksPerPacket=.*, packetSize=.*",
"DEBUG org.apache.hadoop.hdfs.DataStreamer: Queued packet .*",
#JHY
"DEBUG CoGroupedRDD: Adding one\-to\-one dependency with ShuffledRDD\[.*\] at groupByKey at .*",
"DEBUG CoGroupedRDD: Adding one\-to\-one dependency with MapPartitionsRDD\[.*\] at mapValues at .*",
"DEBUG SortShuffleManager: Can't use serialized shuffle for shuffle .* because an aggregator is defined",
"INFO DAGScheduler: Registering RDD .* \(.* at .*\)",
"INFO DAGScheduler: ShuffleMapStage .* \(.* at .*\) finished in .* s",
"DEBUG org.apache.hadoop.hdfs.DataStreamer: DFSClient seqno: .* reply: SUCCESS downstreamAckTimeNanos: .* flag: .*",
#JHY
"DEBUG org.apache.hadoop.hdfs.DataStreamer: DataStreamer block .* sending packet packet seqno: .* offsetInBlock: .* lastPacketInBlock: false lastByteOffsetInBlock: .*",
"DEBUG org.apache.hadoop.hdfs.DataStreamer: DFSClient seqno: .* reply: .* reply: .* downstreamAckTimeNanos: .* flag: .* flag: .*",
"DEBUG org.apache.hadoop.hdfs.DFSClient: DFSClient writeChunk allocating new packet seqno=.*, src=.*, packetSize=.*, chunksPerPacket=.*, bytesCurBlock=.*",
"DEBUG org.apache.hadoop.hdfs.DFSOutputStream: enqueue full packet seqno: .* offsetInBlock: .* lastPacketInBlock: false lastByteOffsetInBlock: .* src=.*, bytesCurBlock=.*, blockSize=.*, appendChunk=.*, .*@\[DatanodeInfoWithStorage\[.*,.*,DISK\], DatanodeInfoWithStorage\[.*,.*,DISK\]\]",
"DEBUG org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl: Processing .* of type .*",
"DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Node being looked for scheduling .* availableResource: <memory:.*, vCores:.*>",
"DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: nodeUpdate: .* clusterResources: <memory:.*, vCores:.*>",
"DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Trying to schedule on node: .* available: <memory:.*, vCores:.*>",
"DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Skip this queue=.*, because it doesn't need more resource, schedulingMode=.* node\-partition=",
"ERROR: New template candidate matches as many as 2 logs previously matched by other templates!!",
"DEBUG org.apache.hadoop.hdfs.DFSOutputStream: enqueue full packet seqno: .* offsetInBlock: .* lastPacketInBlock: false lastByteOffsetInBlock: .* src=.*, bytesCurBlock=.*, blockSize=.*, appendChunk=.*, .*",
"DEBUG org.apache.hadoop.hdfs.server.namenode.NameNodeResourceChecker: Space available on volume 'overlay' is .*",
"DEBUG org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: In memory blockUCState = .*",
"INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: .* = .*",
"DEBUG .* Processing event for .* of type .*",
"DEBUG org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: a metric is reported: cmd: .* user: root \(auth:SIMPLE\)",
"DEBUG org.apache.hadoop.hdfs.server.namenode.FSImageTransactionalStorageInspector: Checking file .*",
"DEBUG org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Rescanning after .* milliseconds",
"DEBUG org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: Processing .* of type .*",
"DEBUG org.apache.hadoop.hdfs.server.namenode.top.TopAuditLogger: \-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\- logged event for top service: allowed=.* ugi=.* \(auth:SIMPLE\) ip=.* cmd=.* src=.* dst=.* perm=.*",
"INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: .* = .*",
"DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: User limit computation for root in queue default userLimitPercent=.* userLimitFactor=.* required: <memory:.*, vCores:.*> consumed: <memory:.*, vCores:.*> user\-limit\-resource: <memory:.*, vCores:.*> queueCapacity: <memory:.*, vCores:.*> qconsumed: <memory:.*, vCores:.*> consumedRatio: .* currentCapacity: <memory:.*, vCores:.*> activeUsers: .* clusterCapacity: <memory:.*, vCores:.*> resourceByLabel: <memory:.*, vCores:.*> usageratio: .* Partition: maxUserLimit=<memory:.*, vCores:.*> userWeight=.*",
"DEBUG org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Reported block .* on .* size .* replicaState = .*",
"INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: .* Container Transitioned from .* to .*",
"DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: post\-assignContainers for application .*",
"INFO org.apache.hadoop.util.GSet: Computing capacity for map .*",
"DEBUG org.apache.hadoop.hdfs.DataStreamer: Closing old block .*",
"INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took .* at .* KB/s",
"INFO org.apache.hadoop.util.GSet: VM type = 64\-bit",
"INFO org.apache.hadoop.hdfs.server.namenode.FSImageTransactionalStorageInspector: No version file in .*",
"INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration: max alloc .* per queue for .* is undefined",
"DEBUG org.apache.hadoop.hdfs.client.impl.DfsClientConf: .* = false",
"INFO org.apache.hadoop.yarn.event.AsyncDispatcher: Registering class .* for class .*",
"INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file .*",
"DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: renaming .* to .*",
"INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting",
"INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: .* State change from .* to .*",
"INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in .* seconds.",
"INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled\? .*",
"DEBUG org.apache.hadoop.hdfs.StateChange: .*",
"DEBUG org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Scanned .* directive\(s\) and .* block\(s\) in .* millisecond\(s\).",
"DEBUG org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Processing event of type .*",
"INFO .* Rolling master\-key for .*",
"DEBUG org.apache.hadoop.hdfs.server.namenode.FSEditLog: .* \[RpcEdit op:AddBlockOp \[path=.*, penultimateBlock=.*, lastBlock=.*, RpcClientId=, RpcCallId=.*\] call:.* Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from .*\]",
"DEBUG org.apache.hadoop.hdfs.server.namenode.FSEditLog: logEdit \[RpcEdit op:SetGenstampV2Op \[GenStampV2=.*, opCode=.*, txid=.*\] call:.* Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from .*\]",
"INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled\? true",
"INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled",
"INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Opening connection to .*",
"INFO org.apache.hadoop.util.GSet: capacity = .* = .* entries",
"DEBUG org.apache.hadoop.hdfs.DataStreamer: Allocating new block",
"INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid .* from .*",
"INFO org.apache.hadoop.util.GSet: .* max memory .* MB = .* MB",
"DEBUG org.apache.hadoop.hdfs.DataStreamer: Send buf size .*",
"DEBUG org.apache.hadoop.hdfs.server.namenode.FSEditLog: .* \[RpcEdit op:SetPermissionsOp \[src=.*, permissions=.*, opCode=.*, txid=.*\] call:.* Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.setPermission from .*\]",
"INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at .*",
"INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port .*",
"INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file .* size .* bytes.",
"INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Storing application with id .*",
"INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occurring more than .* times",
"INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading .* INodes.",
"INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port .*",
"INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on .* acquired by nodename .*",
"INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: .* to active state",
"DEBUG org.apache.hadoop.hdfs.server.namenode.FSEditLog: .* \[RpcEdit op:MkdirOp \[length=.*, inodeId=.*, path=.*, timestamp=.*, permissions=.*, aclEntries=.*, opCode=.*, txid=.*, xAttrs=.*\] call:.* Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.mkdirs from .*\]",
"INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: .* = .*",
"DEBUG org.apache.hadoop.hdfs.util.LightWeightHashSet: initial capacity=.*, max load factor= .* min load factor= .*",
"INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with .* entries .* lookups",
"DEBUG org.apache.hadoop.hdfs.util.MD5FileUtils: Saved MD5 .* to .*",
"DEBUG org.apache.hadoop.hdfs.DataStreamer: Connecting to datanode .*",
"DEBUG org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: PendingReplicationMonitor checking Q",
"INFO org.apache.hadoop.http.HttpRequestLog: Http request log for .* is not defined",
"INFO org.apache.hadoop.http.HttpServer2: Added filter .* \(class=.*\) to context .*",
"INFO org.apache.hadoop.ipc.Server: IPC Server listener on .* starting",
"INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop\-metrics2.properties",
"DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: assignContainers: node=.* #applications=.*",
"INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: .* State change from .* to .* on event=.*",
"INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Increasing replication from .* to .* for .*",
"INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMDelegationTokenSecretManager: storing master key with keyID .*",
"DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration: CSConf \- getCapacity: queuePrefix=.*, capacity=.*",
"INFO org.apache.hadoop.yarn.server.resourcemanager.RMNMInfo: Registered .* MBean",
"DEBUG org.apache.hadoop.hdfs.server.namenode.FSEditLog: logEdit \[RpcEdit op:AllocateBlockIdOp \[blockId=.*, opCode=.*, txid=.*\] call:.* Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.addBlock from .*\]",
"DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: pre\-assignContainers for application .*",
"INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is .*",
"DEBUG org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to choose remote rack \(location = .*\), fallback to local rack",
"INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web\-server for .* at: .*",
"INFO org.apache.hadoop.hdfs.server.namenode.FSImage: No edit log streams selected.",
"INFO org.apache.hadoop.hdfs.server.namenode.Checkpointer: Checkpointer about to load edits from .* stream\(s\).",
"INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in .* msecs",
"INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: true",
"DEBUG org.apache.hadoop.hdfs.web.URLConnectionFactory: open URL connection",
"INFO org.apache.hadoop.hdfs.StateChange: BLOCK\* allocate .* replicas=.*, .* for .*",
"INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Initializing quota with .* thread\(s\)",
"INFO org.apache.hadoop.conf.Configuration: found resource .* at file:.*",
"INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: .* Enabled: .*",
"INFO org.apache.hadoop.yarn.server.resourcemanager.metrics.SystemMetricsPublisher: YARN system metrics publishing service is not enabled",
"INFO org.apache.hadoop.yarn.factories.impl.pb.RpcServerFactoryPBImpl: Adding protocol .* to the server",
"DEBUG org.apache.hadoop.hdfs.server.namenode.FSEditLog: .* \[SyncEdit op:LogSegmentOp \[opCode=.*, txid=.*\]\]",
"DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration: CSConf \- getQueues called for: queuePrefix=.*",
"INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Clear node set for .*",
"INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: .* metrics system started",
"DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration: max alloc .* per queue for .* is .*",
"INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=.* OPERATION=.* Allocated Container TARGET=.* RESULT=.* APPID=.* CONTAINERID=.*",
"INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Detailed lock hold time metrics enabled: false",
"INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing info for app: .*",
"DEBUG org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Removing pending replication for .*",
"INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Edit logging is async:true",
"INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=.*",
"INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Saving image file .* using no compression",
"INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval .* milliseconds",
"INFO SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for .* \(auth:SIMPLE\)",
"INFO org.apache.hadoop.util.JvmPauseMonitor: Starting JVM pause monitor",
"INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: AM registration .*",
"DEBUG org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: blocks = .*",
"INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: checking for deactivate of application :.*",
"DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: allocate: post\-update",
"INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading .* expecting start txid #1",
"DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp: allocate: applicationAttemptId=.* container=.* host=.* type=.*",
"DEBUG org.apache.hadoop.hdfs.DFSClient: Sets dfs.client.block.write.replace\-datanode\-on\-failure.min\-replication to .*",
"ERROR: New template candidate matches as many as 1 logs previously matched by other templates!!",
"INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing .*",
"INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled Metric snapshot period at .* second\(s\).",
"INFO org.apache.hadoop.hdfs.StateChange: DIR\* completeFile: .* is closed by .*",
"DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: allocate: pre\-update .* ask size =.*",
"DEBUG org.apache.hadoop.hdfs.server.namenode.top.window.RollingWindowManager: iterating in reported metrics, size=.*",
"INFO org.apache.hadoop.hdfs.StateChange: STATE\* UnderReplicatedBlocks has .* blocks",
"DEBUG org.apache.hadoop.hdfs.server.namenode.FileJournalManager: FileJournalManager\(root=.*\): selecting input streams starting at .* from among .* candidate file\(s\)",
"DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Trying to assign containers to child\-queue of root",
"DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: printChildQueues \- queue: root child\-queues: .* label=\(\*\)",
"DEBUG org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Trying to retrieve password for .*",
"INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Web server init done",
"INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Allocated new applicationId: .*",
"INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=.*",
"DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt: showRequests: application=.* request={Priority: .* Capability: <memory:.*, vCores:.*>, # Containers: .* Location: \*, Relax Locality: true, Node Label Expression: }",
"WARN org.apache.hadoop.hdfs.server.common.Util: Path .* should be specified as a URI in configuration files. Please update hdfs configuration.",
"DEBUG org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: getDatanodeListForReport with includedNodes = HostSet\(\), excludedNodes = HostSet\(\), foundNodes = HostSet\(\), nodes = .*",
"DEBUG org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager: Created a new BR lease .* for DN .* numPending = .*",
"DEBUG org.apache.hadoop.hdfs.server.namenode.FSEditLog: .* \[RpcEdit op:SetReplicationOp \[path=.*, replication=.*, opCode=.*, txid=.*\] call:.* Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.setReplication from .*\]",
"INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from .*",
"INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: master/.*",
"DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt: showRequests: application=.* headRoom=<memory:.*, vCores:.*> currentConsumption=.*",
"DEBUG org.apache.hadoop.yarn.server.resourcemanager.NodesListManager: .* reported usable",
"INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Updating the current master key for generating delegation tokens",
"DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler: Request for appInfo of unknown attempt .*",
"INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to .*",
"DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: Reloading namespace from .*",
"DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: About to load edits:",
"INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean",
"INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode .*",
"INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.AbstractContainerAllocator: assignedContainer application attempt=.* container=.* queue=.* clusterResource=<memory:.*, vCores:.*> type=.* requestedPartition=",
"INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' \(class=.*\)",
"INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default\-rack/.*",
"DEBUG org.apache.hadoop.hdfs.server.namenode.FSEditLog: .* \[RpcEdit op:AddOp \[length=.*, inodeId=.*, path=.*, replication=.*, mtime=.*, atime=.*, blockSize=.*, .* permissions=.*:supergroup:rw\-r\-\-r\-\-, aclEntries=.*, clientName=.*, clientMachine=.*, overwrite=.*, RpcClientId=.*, RpcCallId=.*, storagePolicyId=.*, opCode=.*, txid=.*\] call:.* Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from .*\]",
"INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: KeyProvider: null",
"INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Quota initialization completed in .* milliseconds",
"DEBUG org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Storing info for attempt: .*",
"INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Creating password for .*",
"INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs",
"DEBUG org.apache.hadoop.hdfs.DataStreamer: DataStreamer block .* sending packet packet seqno: .* offsetInBlock: .* lastPacketInBlock: true lastByteOffsetInBlock: .*",
"DEBUG org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient: SASL client skipping handshake in unsecured configuration for addr = .* datanodeId = DatanodeInfoWithStorage\[.*,.*,DISK\]",
"DEBUG org.apache.hadoop.hdfs.server.namenode.EditLogFileOutputStream: Preallocated .* bytes at the end of the edit log \(offset .*",
"INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around .* .* .*",
"INFO org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl: .* Node Transitioned from NEW to RUNNING",
"DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.ActiveUsersManager: User root .* activeUsers, currently: .*",
"INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to master:.*",
"INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Initialized parent\-queue root name=.*, fullname=.*",
"DEBUG org.apache.hadoop.hdfs.DataStreamer: pipeline = \[DatanodeInfoWithStorage\[.*,.*,DISK\], DatanodeInfoWithStorage\[.*,.*,DISK\]\]",
"DEBUG org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Dest file: .*",
"INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Launching .*",
"INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip\-hostname\-check=.*",
"INFO .* STARTUP_MSG:",
"INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: .* Total time for transactions\(ms\): .* Number of transactions batched in Syncs: .* Number of syncs: .* SyncTimes\(ms\): .*",
"DEBUG org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl: Container .* is the first container get launched for application .*",
"INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain .* images with txid >= .*",
"INFO org.apache.hadoop.yarn.server.resourcemanager.ClientRMService: Application with id .* submitted by user root",
"INFO .* registered UNIX signal handlers for \[TERM, HUP, INT\]",
"INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state",
"INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter\(org.mortbay.log\) via org.mortbay.log.Slf4jLog",
"INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Registering app attempt : .*",
"INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' \(class=.*\)",
"INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockReportLeaseManager: Registered DN .* \(.*\).",
"INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: initializing replication queues",
"DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Assigned to queue: root.default stats: default: capacity=.*, absoluteCapacity=.*, usedResources=<memory:.*, vCores:.*>, usedCapacity=.*, absoluteUsedCapacity=.*, numApps=.*, numContainers=.* \-\-> <memory:.*, vCores:.*>, OFF_SWITCH",
"DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Trying to assign to queue: root.default stats: default: capacity=.*, absoluteCapacity=.*, usedResources=<memory:.*, vCores:.*>, usedCapacity=.*, absoluteUsedCapacity=.*, numApps=.*, numContainers=.*",
"DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Assigned maximum number of off\-switch containers: .* assignments so far: resource:<memory:.*, vCores:.*>; type:OFF_SWITCH; excessReservation:null; applicationid:null; skipped:NONE; fulfilled reservation:false; allocations\(count/resource\):.*/<memory:.*, vCores:.*>; reservations\(count/resource\):.*/<memory:.*, vCores:.*>",
"DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.IncreaseContainerAllocator: Skip allocating increase request since we don't have any increase request on this node=.*",
"DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Headroom calculation for user root: userLimit=<memory:.*, vCores:.*> queueMaxAvailRes=<memory:.*, vCores:.*> consumed=<memory:.*, vCores:.*>",
"DEBUG org.apache.hadoop.yarn.server.resourcemanager.NodesListManager: hostsReader: in= out=",
"DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: allocate: applicationId=.* container=.* host=.* user=.* resource=<memory:.*, vCores:.*> type=.*",
"INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: assignedContainer queue=.* usedCapacity=.* absoluteUsedCapacity=.* used=<memory:.*, vCores:.*> cluster=<memory:.*, vCores:.*>",
"INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re\-sorting assigned queue: root.default stats: default: capacity=.*, absoluteCapacity=.*, usedResources=<memory:.*, vCores:.*>, usedCapacity=.*, absoluteUsedCapacity=.*, numApps=.*, numContainers=.*",
"INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Initializing default",
"DEBUG org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Processing RPC with index .* out of total .* RPCs in processReport .*",
"DEBUG org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: DatanodeManager.addDatanode: node .* is added to datanodeMap.",
"INFO org.apache.hadoop.hdfs.StateChange: BLOCK\* allocate .* replicas=.*, .* for .*",
"INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period :.* secs .* min\)",
"INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Assigned container .* of capacity <memory:.*, vCores:.*> on host .*, which has .* containers, <memory:.*, vCores:.*> used and <memory:.*, vCores:.*> available after allocation",
"INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: NMTokenKeyRollingInterval: .* and NMTokenKeyActivationDelay: .*",
"DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration: CSConf \- getQueues: queuePrefix=.*, queues=.*",
"DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: ParentQ=.* assignedSoFarInThisIteration=<memory:.*, vCores:.*> usedCapacity=.* absoluteUsedCapacity=.*",
"DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator: assignContainers: node=.* application=.* priority=.* request={Priority: .* Capability: <memory:.*, vCores:.*>, # Containers: .* Location: \*, Relax Locality: true, Node Label Expression: } type=.*",
"INFO org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM: Sending NMToken for nodeId : .* for container : .*",
"DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default user=.* used=<memory:.*, vCores:.*> numContainers=.* headroom = <memory:.*, vCores:.*> user\-resources=<memory:.*, vCores:.*>",
"INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: .* => .* success",
"INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: .* scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler",
"INFO org.apache.hadoop.security.Groups: clearing userToGroupsMap cache",
"INFO org.apache.hadoop.hdfs.DataStreamer: Slow ReadProcessor read fields for block .* took .* \(threshold=.*\); ack: seqno: .* reply: .* reply: .* downstreamAckTimeNanos: .* flag: .* flag: .* targets: \[DatanodeInfoWithStorage\[.*,.*,DISK\], DatanodeInfoWithStorage\[.*,.*,DISK\]\]",
"INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Image has changed. Downloading updated image from NN.",
"INFO org.mortbay.log: Started HttpServer2\$SelectChannelConnectorWithSafeStartup@.*",
"INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Image Transfer timeout configured to .* milliseconds",
"INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file .* \-> .*",
"INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger :.* txns",
"INFO org.apache.hadoop.hdfs.StateChange: STATE\* Network topology has .* racks and .* datanodes",
"INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService: NodeManager from node .* httpPort: .* registered with capability: <memory:.*, vCores:.*>, assigned nodeId .*",
"DEBUG org.apache.hadoop.hdfs.server.namenode.FSNamesystem: OP_ADD: .* numblocks : .* clientHolder .* clientMachine .*",
"INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Accepted application .* from user: root, in queue: default",
"DEBUG org.apache.hadoop.hdfs.server.namenode.FSDirectory: Setting quota for",
"DEBUG org.apache.hadoop.hdfs.server.namenode.FSEditLog: logEdit \[ op:null\]",
"DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: LeafQueue: name=.*, fullname=.*",
"INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID .* for DN .*",
"DEBUG org.apache.hadoop.hdfs.client.impl.DfsClientConf: dfs.domain.socket.path =",
"DEBUG org.apache.hadoop.hdfs.server.namenode.FSEditLog: .* \[RpcEdit op:SetGenstampV2Op \[GenStampV2=.*, opCode=.*, txid=.*\] call:Call#9 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.updateBlockForPipeline from .*\]",
"DEBUG org.apache.hadoop.hdfs.server.namenode.FSImage: Summary of operations loaded from edit log:",
"DEBUG org.apache.hadoop.hdfs.server.namenode.FSEditLog: .* \[RpcEdit op:SetOwnerOp \[src=.*, username=.*, groupname=.*, opCode=.*, txid=.*\] call:Call#242 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.setOwner from .*\]",
"DEBUG org.apache.hadoop.hdfs.server.namenode.FSEditLog: .* \[RpcEdit op:CloseOp \[length=.*, inodeId=.*, path=.*, replication=.*, mtime=.*, atime=.*, blockSize=.*, .* permissions=.*:supergroup:rw\-r\-\-r\-\-, aclEntries=.*, clientName=.*, clientMachine=.*, overwrite=.*, storagePolicyId=.*, opCode=.*, txid=.*\] call:Call#12 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.complete from .*\]",
"INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Uploaded image with txid .* to namenode at .* in .* seconds",
"INFO BlockStateChange: BLOCK\* processReport .* Processing first storage report for .* from datanode .*",
"DEBUG org.apache.hadoop.hdfs.server.namenode.FSEditLog: .* \[RpcEdit op:UpdateBlocksOp \[path=.*, .* RpcClientId=.*, RpcCallId=.*\] call:Call#10 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.updatePipeline from .*\]",
"INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: .*",
"INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile\(file=.*, cpktTxId=.*\)",
"DEBUG org.apache.hadoop.hdfs.server.namenode.FSEditLog: .* \[RpcEdit op:CloseOp \[length=.*, inodeId=.*, path=.*, replication=.*, mtime=.*, atime=.*, blockSize=.*, .* permissions=.*:supergroup:rw\-r\-\-r\-\-, aclEntries=.*, clientName=.*, clientMachine=.*, overwrite=.*, storagePolicyId=.*, opCode=.*, txid=.*\] call:Call#22 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.complete from .*\]",
"INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added node .* clusterResource: <memory:.*, vCores:.*>",
"INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: .* container Container: \[ContainerId: .* Version: .* NodeId: .*:.*, NodeHttpAddress: .*:.*, Resource: <memory:.*, vCores:.*>, Priority: .* Token: Token { kind: ContainerToken, service: .* }, \] for AM .*",
"DEBUG org.apache.hadoop.hdfs.client.impl.LeaseRenewer: Lease renewed for client .*",
"INFO org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager: ContainerTokenKeyRollingInterval: .* and ContainerTokenKeyActivationDelay: .*",
"INFO org.apache.hadoop.hdfs.StateChange: BLOCK\* registerDatanode: from DatanodeRegistration\(.*, datanodeUuid=.*, infoPort=.*, infoSecurePort=.*, ipcPort=.*, storageInfo=lv=.*;cid=.*;nsid=.*;c=.*\) storage .*",
"INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Initialized queue: root: numChildQueue= .* capacity=.*, absoluteCapacity=.*, usedResources=<memory:.*, vCores:.*>usedCapacity=.*, numApps=.*, numContainers=.*",
"DEBUG org.apache.hadoop.hdfs.server.namenode.FileJournalManager: selecting edit log stream EditLogFile\(file=.*,first=.*,last=.*,inProgress=.*,hasCorruptHeader=.*\)",
"DEBUG org.apache.hadoop.hdfs.client.impl.LeaseRenewer: Lease renewer daemon for .* with renew id .*",
"DEBUG org.apache.hadoop.hdfs.server.namenode.FSEditLog: logSync \[RpcEdit op:RenameOldOp \[length=.*, src=.*, dst=.*, timestamp=.*, RpcClientId=, RpcCallId=.*, opCode=.*, txid=.*\] call:Call#17 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.rename from .*\]",
"DEBUG org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Renaming .* to .*",
"INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=.* IP=.* OPERATION=.* Application Request TARGET=.* RESULT=.* APPID=.*",
"INFO org.apache.hadoop.hdfs.StateChange: STATE\* Leaving safe mode after .* secs",
"INFO org.mortbay.log: jetty\-6.1.26",
"DEBUG org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Will connect to NameNode at .*",
"INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use .* of total heap and retry cache entry expiry time is .* millis",
"INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Image file .* of size .* bytes saved in .* seconds.",
"INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Create AMRMToken for ApplicationAttempt: .*",
"INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.",
"INFO org.apache.hadoop.http.HttpServer2: adding path spec: .*",
"INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=.* IP=.* OPERATION=.* App Master TARGET=.* RESULT=.* APPID=.* APPATTEMPTID=.*",
"INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use master:.* to access this namenode/service.",
"INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application .* from user: root activated in queue: default",
"INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image\? false \(staleImage=.*, haEnabled=.*, isRollingUpgrade=.*\)",
"INFO org.apache.hadoop.conf.Configuration: dynamic\-resources.xml not found",
"INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Using Scheduler: org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler",
"DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: setChildQueues: .* label=\(\*\)",
"DEBUG org.apache.hadoop.hdfs.server.namenode.FileJournalManager: passing over EditLogFile\(file=.*,first=.*,last=.*,inProgress=.*,hasCorruptHeader=.*\) because it is in progress and we are ignoring in\-progress logs.",
"WARN org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: The specific max attempts: .* for application: .* is invalid, because it is out of the range .* Use the global max attempts instead.",
"INFO org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager: Starting expired delegation token remover thread, tokenRemoverScanInterval=.* min\(s\)",
"WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory \(dfs.namenode.edits.dir\) configured. Beware of data loss due to lack of redundant storage directories!",
"DEBUG org.apache.hadoop.hdfs.server.namenode.FSEditLog: logSync \[RpcEdit op:AddOp \[length=.*, inodeId=.*, path=.*, replication=.*, mtime=.*, atime=.*, blockSize=.*, blocks=.*, permissions=.*, aclEntries=.*, clientName=.*, clientMachine=.*, overwrite=.*, RpcClientId=, RpcCallId=.*, storagePolicyId=.*, opCode=.*, txid=.*\] call:Call#3 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.create from .*\]",
"INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating AMRMToken",
"DEBUG org.apache.hadoop.hdfs.DFSClient: .* masked=.*",
"INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file .* of size .* edits # .* loaded in .* seconds",
"INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application added \- appId: .* user: root leaf\-queue of parent: root #applications: .*",
"DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: application .* AMResource <memory:.*, vCores:.*> maxAMResourcePerQueuePercent .* amLimit <memory:.*, vCores:.*> lastClusterResource <memory:.*, vCores:.*> amIfStarted <memory:.*, vCores:.*> AM node\-partition name",
"DEBUG org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Using blacklist for AM: .* and .*",
"DEBUG org.apache.hadoop.hdfs.server.namenode.FSNamesystem: OP_ADD_BLOCK: .* new block id : .*",
"WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory \(dfs.namenode.name.dir\) configured. Beware of data loss due to lack of redundant storage directories!",
"DEBUG org.apache.hadoop.hdfs.server.namenode.FSEditLog: logEdit \[RpcEdit op:CloseOp \[length=.*, inodeId=.*, path=.*, replication=.*, mtime=.*, atime=.*, blockSize=.*, .* permissions=.*:supergroup:.*, aclEntries=.*, clientName=.*, clientMachine=.*, overwrite=.*, storagePolicyId=.*, opCode=.*, txid=.*\] call:.* Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.complete from .*\]",
"INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Added Application Attempt .* to scheduler from user root in queue default",
"INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application .* is submitted without priority hence considering default queue/cluster priority: .*",
"DEBUG org.apache.hadoop.hdfs.server.namenode.FSEditLog: logSync \[RpcEdit op:CloseOp \[length=.*, inodeId=.*, path=.*, replication=.*, mtime=.*, atime=.*, blockSize=.*, blocks=.*, permissions=.*, aclEntries=.*, clientName=.*, clientMachine=.*, overwrite=.*, storagePolicyId=.*, opCode=.*, txid=.*\] call:.* Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.complete from .*\]",
"INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: .* AttemptId: .* MasterContainer: Container: \[ContainerId: .* Version: .* NodeId: .*:.*, NodeHttpAddress: .*:.*, Resource: <memory:.*, vCores:.*>, Priority: .* Token: Token { kind: ContainerToken, service: .* }, \]",
"WARN org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint done. New Image Size: .*",
"INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: AMRMTokenKeyRollingInterval: .* and AMRMTokenKeyActivationDelay: .* ms",
"DEBUG org.apache.hadoop.hdfs.server.namenode.FSEditLog: Generated manifest for logs since .*",
"INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Initialized root queue root: numChildQueue= .* capacity=.*, absoluteCapacity=.*, usedResources=<memory:.*, vCores:.*>usedCapacity=.*, numApps=.*, numContainers=.*",
"INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: root, capacity=.*, asboluteCapacity=.*, maxCapacity=.*, asboluteMaxCapacity=.*, state=.*, acls=.*:\*SUBMIT_APP:\*, labels=.*,, offswitchPerHeartbeatLimit = .* reservationsContinueLooking=.*",
"INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application added \- appId: .* user: root, leaf\-queue: default #user\-pending\-applications: .* #user\-active\-applications: .* #queue\-pending\-applications: .* #queue\-active\-applications: .*",
'''DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Effective user AM limit for "root":<memory:.*, vCores:.*>. Effective weighted user AM limit: <memory:.*, vCores:.*>. User weight: .*''',
"DEBUG org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil: DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection",
"INFO BlockStateChange: BLOCK\* processReport .* from storage .* node DatanodeRegistration\(.*, datanodeUuid=.*, infoPort=.*, infoSecurePort=.*, ipcPort=.*, storageInfo=lv=.*;cid=.*;nsid=.*;c=.*\), blocks: .* hasStaleStorage: false, processing time: .* msecs, invalidatedBlocks: .*",
"INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in .*",
"INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment .*",
"INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Initialized queue mappings, override: false",
"INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Initialized CapacityScheduler with calculator=.* org.apache.hadoop.yarn.util.resource.DefaultResourceCalculator, minimumAllocation=<<memory:.*, vCores:.*>>, maximumAllocation=<<memory:.*, vCores:.*>>, asynchronousScheduling=.*, asyncScheduleInterval=.*",
"INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: .* newGS=.*, newLength=.*, newNodes=\[.*\], client=.*\)",
"INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Priority .* is acceptable in queue : default for application: .* for the user: root",
"INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Initialized queue: default: capacity=.*, absoluteCapacity=.*, usedResources=<memory:.*, vCores:.*>, usedCapacity=.*, absoluteUsedCapacity=.*, numApps=.*, numContainers=.*",
"DEBUG org.apache.hadoop.hdfs.server.namenode.FSEditLog: logEdit \[RpcEdit op:RenameOldOp \[length=.*, src=.*, dst=.*, timestamp=.*, RpcClientId=.*, RpcCallId=.*, opCode=.*, txid=.*\] call:Call#17 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.rename from .*\]",
"INFO org.apache.hadoop.yarn.webapp.WebApps: Registered webapp guice modules",
"INFO org.apache.hadoop.hdfs.StateChange: STATE\* Replication Queue initialization scan for invalid, over\- and under\-replicated blocks completed in .* msec",
"DEBUG org.apache.hadoop.hdfs.server.blockmanagement.DecommissionManager: Activating DecommissionManager with interval .* seconds, .* max blocks per interval, .* max nodes per interval, .* max concurrently tracked nodes.",
"DEBUG org.apache.hadoop.hdfs.DataStreamer: Waiting for ack for: .*",
"INFO org.apache.hadoop.hdfs.server.common.Storage: Storage directory .* has been successfully formatted.",
"INFO org.apache.hadoop.yarn.webapp.WebApps: Web app cluster started at .*",
"INFO org.apache.hadoop.yarn.security.YarnAuthorizationProvider: org.apache.hadoop.yarn.security.ConfiguredYarnAuthorizer is instiantiated.",
"INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=.*;org.apache.hadoop.hdfs.web.resources, pathSpec=.*",
"DEBUG org.apache.hadoop.hdfs.DFSOutputStream: Closing an already closed stream. \[Stream:true, streamer:true\]",
"INFO org.apache.hadoop.util.HostsFileReader: Refreshing hosts \(include/exclude\) list",
"INFO org.mortbay.log: Extract jar:file:.*!/webapps/cluster to .*",
"INFO org.apache.hadoop.util.GSet: .* max memory .* MB = .* KB",
"INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG:",
"WARN org.apache.hadoop.conf.Configuration: core\-site.xml:an attempt to override final parameter: fs.defaultFS; Ignoring.",
"INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Allocated new BlockPoolId: .*",
]

templatebyYJH= [ 
"DEBUG Configuration: Handling deprecation for .*",
"INFO Client: Uploading resource file:.*-> .*",
"DEBUG Client: IPC Client .* connection to .* from .* sending .*",
"DEBUG Client: IPC Client .* connection to .* from .* got value .*",
"DEBUG ProtobufRpcEngine: Call: getFileInfo took .*",
"INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=.* OPERATION=.* Allocated Container        TARGET=.*     RESULT=.*  APPID=.*    CONTAINERID=.*",
"DEBUG DFSClient: computePacketChunkSize: src=.*, chunkSize=.*, chunksPerPacket=.*, packetSize=.*",
"DEBUG ClosureCleaner:.*",
"DEBUG DFSClient: DFSClient writeChunk allocating new packet seqno=.*, src=.*, packetSize=.*, chunksPerPacket=.*, bytesCurBlock=.*",
"DEBUG DFSClient: DFSClient writeChunk packet full seqno=.*, src=.*, bytesCurBlock=.*, blockSize=.*, appendChunk=.*",
"DEBUG DFSClient: Queued packet .*",
"INFO Client: Uploading resource file:.* -> .*",
"DEBUG CodeGenerator:.*",
"DEBUG BaseSessionStateBuilder.*:.*",
"DEBUG WholeStageCodegenExec:.*",
"INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef\(.*\) \(.*\) with ID .*",
"INFO CodeGenerator: Code generated in .*",
"DEBUG DFSClient: Waiting for ack for:.*",
"DEBUG Client: stopping client from cache:.*",
"DEBUG DFSClient: DataStreamer block .* sending packet packet seqno: .* offsetInBlock: .* lastPacketInBlock: .* lastByteOffsetInBlock: .*",
"DEBUG DFSClient: DFSClient seqno: .* reply: .* reply: .* downstreamAckTimeNanos: .* flag: .* flag: .*",
"DEBUG ContextCleaner: Cleaning accumulator .*",
"DEBUG ContextCleaner: Got cleaning task CleanAccum\(.*\)",
"INFO ContextCleaner: Cleaned accumulator .*",
"DEBUG ContextCleaner: Cleaned broadcast .*",
"DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: User limit computation for .* in queue default userLimitPercent=.* userLimitFactor=.* required: <memory:.*, vCores:.*> consumed: <memory:.*, vCores:.*> user-limit-resource: <memory:.*, vCores:.*> queueCapacity: <memory:.*, vCores:.*> qconsumed: <memory:.*, vCores:.*> consumedRatio: .* currentCapacity: <memory:.*, vCores:.*> activeUsers: .* clusterCapacity: <memory:.*, vCores:.*> resourceByLabel: <memory:.*, vCores:.*> usageratio: .* Partition:  maxUserLimit=<memory:.*, vCores:.*> userWeight=.*",
"DEBUG DFSClient: DFSClient seqno: .* reply: .* downstreamAckTimeNanos: .* flag: .*",
"DEBUG org.apache.hadoop.hdfs.server.namenode.top.TopAuditLogger: ------------------- logged event for top service: .*",
"INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=.*OPERATION=.*TARGET=.*RESULT=.*APPID=.*",
"DEBUG org.apache.hadoop.hdfs.server.namenode.FSEditLog: logSync \[RpcEdit op:.* \[length=.*, src=.*, dst=.*, timestamp=.*, RpcClientId=.*, RpcCallId=.*, opCode=.*, txid=.*\] call:.* Retry.* org.apache.hadoop.hdfs.protocol.ClientProtocol.rename from .*\]",
"INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler: Container .* completed with event .*, but corresponding RMContainer doesn't exist.",
"DEBUG ProtobufRpcEngine: Call: getApplicationReport took .*",
"INFO TaskSetManager: Finished task .* in stage .* \(.*\) in .* on .* \(.*\) \(.*\)",
"INFO BlockManagerInfo: Removed .* on .*:.* in memory \(size: .*, free: .*\)",
"DEBUG BlockManager: Getting remote block .*",
"DEBUG YarnScheduler: parentName:.*, name: .*, runningTasks: .*",
"INFO BlockManagerInfo: Added .* in memory on .* \(size: .*, free:.*\)",
"DEBUG MapOutputTrackerMaster: Handling request to send map output locations for shuffle .* to .*",
"INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle .* to .*",
"INFO TaskSetManager: Starting task .* in stage .* \(TID .*, .*, executor .*, partition .*, .*, .*\)",
"INFO Client: Application report for .* \(state: .*\)",
"INFO org.apache.hadoop.util.GSet:.*",
"DEBUG org.apache.hadoop.hdfs.server.namenode.FSEditLog: logEdit \[RpcEdit op:.* \[.*\]",
"DEBUG Client:",
"DEBUG Recycler: .*: .*",
"DEBUG PooledByteBufAllocator: .*: .*",
"DEBUG PlatformDependent.*: .*: .*",
"INFO SecurityManager: Changing .* acls.*to:.*",
"INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Log Size Trigger.*:.*",
"INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around .*",
"DEBUG org.apache.hadoop.hdfs.server.namenode.FSEditLog: logSync \[RpcEdit op:.* \[.*\]",
"INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Cleaning .*",
"DEBUG org.apache.hadoop.yarn.server.resourcemanager.RMAppManager: RMAppManager processing event for .* of type .*",
"INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: Unregistering app attempt : .*",
"INFO SparkContext: .*",
"DEBUG ByteBufUtil: .*: .*",
"DEBUG NetUtil: .*: .*",
"DEBUG ResourceLeakDetector: .*: .*",
"DEBUG InternalThreadLocalMap: .*: .*",
"DEBUG DefaultChannelId: .*: .*",
"DEBUG TaskResultGetter: Fetching indirect task result for .*",
"DEBUG DAGScheduler: .*: .*",
"DEBUG DAGScheduler: .*\(.*\)",
"DEBUG YarnSchedulerBackend\$YarnDriverEndpoint: Launching task .*",
"DEBUG DAGScheduler: ShuffleMapTask finished on .*",
"INFO JettyUtils: Adding filter .* to .*",
"DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Headroom calculation for user .*:  userLimit=<memory:.*, vCores:.*> queueMaxAvailRes=<memory:.*, vCores:.*> consumed=<memory:.*, vCores:.*>",
"INFO org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: .*unregistered successfully.*",
"INFO ShutdownHookManager: .*",
"INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Checkpoint Period.*:.*",
"DEBUG NioEventLoop: .*: .*",
"INFO SparkEnv: Registering .*",
"DEBUG BlockReaderLocal: .* = .*",
"DEBUG ProtobufRpcEngine: .*: .*",
"DEBUG BlockManagerSlaveEndpoint: Sent response: .* to master:.*",
"DEBUG TransportClient: Sending fetch chunk request .* to .*",
"DEBUG LeaseRenewer: Lease renewer daemon for .* with renew id .*",
"INFO DAGScheduler: .*: .*",
"DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Re-sorting completed queue: default: capacity=.*, absoluteCapacity=.*, usedResources=.*, usedCapacity=.*, absoluteUsedCapacity=.*, numApps=.*, numContainers=.*",
"DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: completedContainer root: numChildQueue= .*, capacity=.*, absoluteCapacity=.*, usedResources=.*usedCapacity=.*, numApps=.*, numContainers=.*, cluster=.*",
"INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode: Released container .* of capacity .* on host .*:.*, which currently has .* containers, .* used and .* available, release resources=.*",
"DEBUG BlockManager: Removing .*",
"DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: default used=.* numContainers=.* user=.* user-resources=.*",
"INFO org.apache.hadoop.yarn.server.resourcemanager.security.AMRMTokenSecretManager: Application finished, removing password for .*",
"INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application .* requests cleared",
"INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Updating application .* with final state: .*",
"INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Updating application attempt .* with final state: .*, and exit status: .*",
"INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Done launching container Container: \[ContainerId: .*, Version: .*, NodeId: .*, NodeHttpAddress: .*, Resource: .*, Priority: .*, Token: .* \] for AM .*",
"INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo: Application .* requests cleared",
"DEBUG ContextCleaner: Cleaning .*",
"DEBUG ContextCleaner: Got cleaning task .*\(.*\)",
"DEBUG MemoryStore: Block .* of size .* dropped from memory \(.*\)",
"DEBUG BlockManagerMaster: Updated info of block .*",
"INFO MemoryStore: MemoryStore cleared",
"INFO BlockManager: BlockManager stopped",
"INFO BlockManagerMaster: BlockManagerMaster stopped",
"INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!",
"DEBUG org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating info for attempt: .*",
"INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Updating info for app: .*",
"INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application Attempt .* is done. finalState=.*",
"INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Application removed \- appId: .* user: .* leaf\-queue of parent:.*applications: .*",
"DEBUG BlockManager: Level for block .* is StorageLevel\(.*\)",
"DEBUG BlockManager: Getting local block .* as bytes",
"DEBUG TaskSetManager: Moving to .* after waiting for .*",
"INFO DAGScheduler: Submitting .* \(.* at .* at .*\), which has no missing parents",
"INFO YarnScheduler: Adding task set .* with .*",
"DEBUG TaskSetManager: Epoch for TaskSet .*: .*",
"DEBUG DAGScheduler: After removal of stage .*, remaining stages = .*",
"DEBUG BlockManagerSlaveEndpoint: Done removing .*, response is .*",
"INFO YarnClientSchedulerBackend: Shutting down all executors",
"INFO TransportClientFactory: Successfully created connection to .* after .* \(.*\)",
"DEBUG TransportClientFactory: Creating new connection to .*",
"DEBUG TransportClientFactory: Connection to .* successful, running .*",
"DEBUG org.apache.hadoop.hdfs.server.namenode.FSNamesystem: OP_CLOSE: .* numblocks : .* clientHolder  clientMachine",
"DEBUG MultithreadEventLoopGroup: .*: .*",
"INFO DiskBlockManager: Created local directory at .*",
"DEBUG RetryUtils: .* = .*",
"DEBUG DFSClient: Connecting to datanode .*",
"INFO MemoryStore: MemoryStore started with capacity .*",
"DEBUG DFSClient: pipeline = DatanodeInfoWithStorage\[.*\]",
"DEBUG DFSClient: .*: masked=.*",
"DEBUG DFSClient: Send buf size .*",
"INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Setting up container Container: \[ContainerId: .*, Version: .*, NodeId: .*, NodeHttpAddress: .*, Resource: .*, Priority: .*, Token: .* \] for AM .*",
"INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue: Application removed \- appId: .* user: .* queue: .* .*applications: .* .*applications: .* .*applications: .* .*applications: .*",
"WARN DFSClient: DFSOutputStream ResponseProcessor exception  for block .*",
"DEBUG SaslDataTransferClient: SASL client skipping handshake in unsecured configuration for addr = .*, datanodeId = .*",
"DEBUG DFSClient: Allocating new block",
"DEBUG DFSClient: Closing old block .*",
"DEBUG TransportServer: Shuffle server started on port: .*",
"DEBUG HadoopDelegationTokenManager: Service .* does not require a token. Check your configuration to see if security is disabled or not.*",
"DEBUG TorrentBroadcast: Unpersisting TorrentBroadcast .*",
"DEBUG BlockManager: Told master about block .*",
"DEBUG BlockManagerSlaveEndpoint: removing broadcast .*",
"INFO MemoryStore: Block .* stored as .* in memory \(.*\)",
"DEBUG TaskSetManager: No tasks for locality level .*, so moving to locality level .*",
"INFO FileSourceStrategy: .*:.*",
"INFO FileSourceScanExec: .*:.*",
"DEBUG BlockManager: Put block .* locally took  .*",
"DEBUG BlockManager: Putting block .* without replication took  .*",
"INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Storing attempt: AppId: .* AttemptId: .* MasterContainer: Container: \[ContainerId: .*, Version: .*, NodeId: .*, NodeHttpAddress: .*, Resource: .*, Priority: .*, Token: .*, \]",
"INFO org.apache.hadoop.yarn.server.resourcemanager.RMAppManager\$ApplicationSummary: appId=.*,name=.*,user=.*,queue=.*,state=.*,trackingUrl=.*,appMasterHost=.*,startTime=.*,finishTime=.*,finalStatus=.*,memorySeconds=.*,vcoreSeconds=.*,preemptedMemorySeconds=.*,preemptedVcoreSeconds=.*,preemptedAMContainers=.*,preemptedNonAMContainers=.*,preemptedResources=.*,applicationType=.*",
"DEBUG TaskSetManager: Valid locality levels for .*: .*, .*, .*",
"INFO BlockManagerMasterEndpoint: Registering block manager .* with .* RAM, BlockManagerId\(.*\)",
"INFO DAGScheduler: Submitting .* from .* \(.* at .* at .*\) \(.* are for partitions .*\)",
"INFO DAGScheduler: ResultStage .* \(collect at .*\) finished in .*",
"INFO NettyBlockTransferService: Server created on .*",
"INFO BlockManagerMaster: Registering BlockManager .*\(.*\)",
"INFO YarnSchedulerBackend\$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef\(.*\) \(.*\) with ID .*",
"DEBUG BlockManagerSlaveEndpoint: removing shuffle .*",
"INFO SharedState: Warehouse path is .*",
"INFO YarnClientSchedulerBackend: Application .* has started running\.",
"INFO SchedulerExtensionServices: Starting Yarn extension services with app .* and attemptId .*",
"INFO YarnClientImpl: Submitted application .*",
"INFO Client: Submitting application .* to .*",
"INFO Client: Will allocate AM container, with .* memory including .* overhead",
"INFO RMProxy: Connecting to ResourceManager at .*",
"DEBUG AbstractService: Service .* is started",
"INFO Client: Requesting a new application from cluster with .* NodeManagers",
"INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster \(.* per container\)",
"DEBUG AbstractByteBuf: .*: .*",
"DEBUG ResourceLeakDetectorFactory: Loaded .*: .*",
"DEBUG LeaseRenewer: Lease renewed for client .*",
"DEBUG AbstractService: Service .* is started",
"DEBUG YarnRPC: Creating YarnRPC for .*",
"DEBUG HBaseDelegationTokenProvider: Fail to invoke .*",
"DEBUG Server: rpcKind=.*, rpcRequestWrapperClass=.*, rpcInvoker=.*",
"DEBUG DataTransferSaslUtil: DataTransferProtocol not using .*, no QOP found in configuration for .*",
"WARN NativeCodeLoader: Unable to load native-hadoop library for your platform.* using builtin\-java classes where applicable",
"DEBUG SecurityManager: Created SSL options for ui: SSLOptions{enabled=.*, port=.*, keyStore=.*, keyStorePassword=.*, trustStore=.*, trustStorePassword=.*, protocol=.*, enabledAlgorithms=.*}",
"INFO Utils: Successfully started service .* on port .*",
"WARN DFSClient: Error Recovery for block .*:.* in pipeline DatanodeInfoWithStorage\[.*,.*,.*\], DatanodeInfoWithStorage\[.*,.*\]: bad datanode DatanodeInfoWithStorage\[.*\]",
"INFO BlockManagerMaster: Registered BlockManager BlockManagerId\(.*, .*, .*, .*\)",
"DEBUG DefaultTopologyMapper: Got a request for .*",
"INFO DAGScheduler: Registering .* \(mapToPair at .*\)",
"INFO SparkUI: Stopped Spark web UI at .*",
"INFO YarnClientSchedulerBackend: Stopped",
"INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: .*",
"INFO SparkUI: Bound SparkUI to .*, and started at .*",
"DEBUG HadoopYarnProtoRPC: Creating a HadoopYarnProtoRpc proxy for protocol interface .*",
"INFO YarnClientSchedulerBackend: Add WebUI Filter. .*, Map\(PROXY_HOSTS -> .*, PROXY_URI_BASES -> .*\), .*",
"INFO BlockManager: Initialized BlockManager: BlockManagerId\(.*, .*, .*, .*\)",
"INFO YarnScheduler: Removed TaskSet .*, whose tasks have all completed, from pool",
"DEBUG AbstractService: Service: .* entered state .*",
"INFO BlockManagerMasterEndpoint: Using .* for getting topology information",
"DEBUG MapOutputTrackerMasterEndpoint: init",
"DEBUG SecurityManager: Created SSL options for fs: SSLOptions{enabled=.*, port=.*, keyStore=.*, keyStorePassword=.*, trustStore=.*, trustStorePassword=.*, protocol=.*, enabledAlgorithms=.*}",
"INFO SharedState: Setting hive.metastore.warehouse.dir .* to the value of spark.sql.warehouse.dir .*",
"INFO Client: Setting up .* for .*",
"INFO Client: Preparing .* for .*",
"WARN Client: Neither .* is set, falling back to uploading libraries under .*",
"DEBUG UserGroupInformation: PrivilegedAction as:.*  from:.*",
"DEBUG UserGroupInformation: PrivilegedAction as:.* \(.*\) from:.*\(.*\)",
"DEBUG SparkContext: Adding shutdown hook",
"INFO ContextCleaner: Cleaned shuffle .*",
"INFO YarnSchedulerBackend\$YarnSchedulerEndpoint: .* registered as .*",
"DEBUG SortShuffleManager: Can't use serialized shuffle for shuffle .* because the serializer, org.apache.spark.serializer.JavaSerializer, does not support object relocation",
"DEBUG MapOutputTrackerMaster: Increasing epoch to .*",
"INFO YarnClientSchedulerBackend: Interrupting monitor thread",
"INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!",
"INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: .*; groups with view permissions: .*; users  with modify permissions: .*; groups with modify permissions: .*",
"INFO BlockManager: Using .* for block replication policy",
"INFO DAGScheduler: ShuffleMapStage .* \(mapToPair at .*\) finished in .*",
"INFO DAGScheduler: looking for newly runnable stages",
"DEBUG DiskBlockManager: Adding shutdown hook",
"DEBUG InternalLoggerFactory: Using .* as the default logging framework",
"DEBUG CleanerJava6: .*: .*",
"DEBUG SparkEnv: Using serializer: class .*",
"INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up",
"INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint",
"DEBUG Analyzer\$ResolveReferences: Resolving .* to .*",
"INFO DAGScheduler: Got job .* \(.* at .*\) with .* output partitions",
"WARN org.apache.hadoop.conf.Configuration: core-site.xml:an attempt to override final parameter: .*  Ignoring.",
"INFO SchedulerExtensionServices: Stopping .*",
"INFO YarnSchedulerBackend\$YarnDriverEndpoint: Asking each executor to shut down",
"DEBUG YarnClientSchedulerBackend: ClientArguments called with: \-\-arg .*",
"DEBUG PerformanceAdvisory: Both short-circuit local reads and UNIX domain socket are disabled.",
"INFO OutputCommitCoordinator\$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!",
"DEBUG OutputCommitCoordinator\$OutputCommitCoordinatorEndpoint: init",
"DEBUG HadoopDelegationTokenManager: Using the following delegation token providers: .*"
]

errored = [
"INFO DAGScheduler: ResultStage .* \(reduce at .*\) finished in .* s",
"ERROR TransportRequestHandler: Error while invoking .* for .*",
"DEBUG org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager: Exporting access keys",
"DEBUG org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager: Generating block token for block_token_identifier \(expiryDate=.*, keyId=.*, userId=.*, blockPoolId=.*, blockId=.*, access modes=.*\)",
"INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.key.update.interval=.*, dfs.block.access.token.lifetime=.*, dfs.encrypt.data.transfer.algorithm=.*",
"DEBUG TaskSetManager: Valid locality levels for TaskSet .*: .*",
"WARN TaskSetManager: Stage .* contains a task of very large size .* The maximum recommended task size is .*",
"DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator: cannot allocate required resource=<memory:.*, vCores:.*> because of headroom",
"DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Decrease parentLimits <memory:.*, vCores:.*> for .* by <memory:.*, vCores:.*> as childQueue=.* is .*",
"INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine \(eg GC\): pause of approximately .*ms",
"ERROR SparkContext: Error initializing SparkContext.",
"WARN DFSClient: DataStreamer Exception",
"DEBUG org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue: Assigned to queue: .* stats: .* capacity=.*, absoluteCapacity=.*, usedResources=<memory:.*, vCores:.*>, usedCapacity=.*, absoluteUsedCapacity=.*, numApps=.*, numContainers=.* --> <memory:.*, vCores:.*>, .*",
"DEBUG org.apache.hadoop.hdfs.client.impl.DfsClientConf: dfs.client.use.legacy.blockreader.local = .*",
"INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Stopping services started for active state",
"ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: Failed to start namenode.",
"INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Determined nameservice ID: .*",
"INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: FSNamesystem write lock held for .* ms via",
"INFO org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl: Container .* already scheduled for cleanup, no further processing",
"INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: .* enabled? .*",
"INFO org.apache.hadoop.ipc.Client: Retrying connect to server: .* Already tried .* time\(s\); retry policy is .*\(maxRetries=.*, sleepTime=.* MILLISECONDS\)",
"ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: Exception in doCheckpoint",
"WARN org.apache.hadoop.ipc.Client: Failed to connect to server: .*: retries get failed due to exceeded maximum allowed retries number: .*",
"WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is .* but only .* storage types can be selected .*",
"INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Stopping services started for standby state",
"ERROR org.apache.hadoop.hdfs.server.namenode.FSNamesystem: FSNamesystem initialization failed.",
"DEBUG Dispatcher: Message .*\(.*\) dropped. RpcEnv already stopped.",
"WARN MetricsSystem: Stopping a MetricsSystem that is not running",
"INFO Client: Deleted staging directory .*",
"ERROR org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: error reading hosts files:",
"DEBUG org.apache.hadoop.hdfs.server.namenode.FSNamesystem: .*: /user/root/.sparkStaging/application_.*/__spark_libs__.*.zip numblocks : .*",
"WARN org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl: Container .* was running but not reported from .*:.*",
"INFO org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: SHUTDOWN_MSG:",
"INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: false",
"WARN Client: Failed to cleanup staging dir .*",
"INFO org.apache.hadoop.io.compress.CodecPool: Got brand-new .* \[.deflate\]",
"DEBUG org.apache.hadoop.hdfs.server.namenode.FSEditLog: Closing log when already closed",
"INFO org.apache.hadoop.hdfs.StateChange: .* allocate .*, replicas=.* for /user/.*/.sparkStaging/application_.*/__spark_libs__.*.zip",
"WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: NameNode low on available disk space. Already in safe mode.",
"INFO org.apache.hadoop.hdfs.StateChange: .* Safe mode is .* are low on .* Please add or free up more resources then turn off safe mode manually. NOTE:  If you turn off safe mode before adding resources, the .* will immediately return to safe mode. Use \"hdfs dfsadmin -safemode leave\" to turn safe mode off.",
"ERROR org.apache.hadoop.hdfs.server.common.Util: Syntax error in URI \$\{dfs.namenode.name.* Please check hdfs configuration.",
"INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Done logSyncAll lastWrittenTxId=.* lastSyncedTxid=.* mostRecentTxid=.*",
"INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: logSyncAll toSyncToTxId=.* lastSyncedTxid=.* mostRecentTxid=.*",
"INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Edit logging is async:false",
"INFO org.apache.hadoop.hdfs.server.blockmanagement.HeartbeatManager: Setting heartbeat recheck interval to .* since .* is less than .*",
"INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1",
"DEBUG DFSClient: Using legacy short-circuit local reads.",
"WARN YarnSchedulerBackend\$YarnSchedulerEndpoint: Attempted to request executors before the AM has registered!",
"DEBUG org.apache.hadoop.hdfs.client.impl.DfsClientConf: .* = .*",
"INFO org.apache.hadoop.ipc.Server: IPC Server handler .* on .*, call Call#.* Retry#.* .* from .*",
"WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of .* to reach .* \(unavailableStorages=\[\], storagePolicy=BlockStoragePolicy{HOT:.*, storageTypes=\[.*\], creationFallbacks=\[\], replicationFallbacks=\[.*\]}, newBlock=.*\)",
"INFO DFSClient: Exception in createBlockOutputStream",
"INFO DFSClient: Abandoning .*",
"INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system .*",
"ERROR Utils: Uncaught exception in thread main",
"INFO org.mortbay.log: Stopped .*",
"INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is disabled",
"INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Detailed lock hold time metrics enabled: true",
"INFO org.apache.hadoop.http.HttpServer2: HttpServer.start\(\) threw a non Bind IOException",
"ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: Unable to load NameNode plugins. Specified list of plugins: .*",
"WARN org.apache.hadoop.hdfs.server.common.Storage: set restore failed storage to true",
"INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: FSEditLogAsync was interrupted, exiting",
"DEBUG org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Namenode is in safemode, skipping scrubbing of corrupted lazy-persist files.",
"WARN org.apache.hadoop.hdfs.DFSUtil: Exception in creating socket address .*",
"DEBUG org.apache.hadoop.hdfs.server.namenode.NameNode: Setting fs.defaultFS to .*",
"INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to .*",
"WARN org.apache.hadoop.http.HttpServer2: HttpServer Acceptor: isRunning is false.*",
"WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Configured NNs:",
"INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: .* enabled\? .*",
"INFO org.apache.hadoop.security.http.RestCsrfPreventionFilter: Adding cross-site request forgery \(.*\) protection, headerName = .*, methodsToIgnore = \[TRACE, HEAD, GET, OPTIONS\], browserUserAgents = \[.*, .*\]",
"ERROR Client: Failed to contact YARN for application .*",
"ERROR YarnClientSchedulerBackend: Yarn application has already exited with state .*",
"INFO org.apache.hadoop.hdfs.StateChange: STATE\* Safe mode .*",
"INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Stopping NameNode metrics system...",
"WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Encountered exception loading fsimage",
"INFO org.apache.hadoop.hdfs.tools.DFSZKFailoverController: SHUTDOWN_MSG:",
"DEBUG org.apache.hadoop.hdfs.tools.DFSHAAdmin: Using NN principal:",
"INFO DFSClient: Excluding datanode DatanodeInfoWithStorage\[.*,.*,DISK\]",
"DEBUG DFSClient: Getting new encryption token from NN",
"ERROR org.apache.hadoop.hdfs.server.common.Util: Syntax error in URI \$\{.* Please check hdfs configuration.",
"INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Saving image file .* using codec .*",
"INFO org.apache.hadoop.io.compress.zlib.ZlibFactory: Successfully loaded & initialized native-zlib library",
"DEBUG org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to choose from local rack \(location = .*\); the second replica is not found, retry choosing ramdomly",
"ERROR TransportRequestHandler: Error sending result RpcResponse{requestId=.*, body=NioManagedBuffer{buf=.*\[pos=.* lim=.* cap=.*\]}} to .*; closing connection",
"INFO org.apache.hadoop.ipc.Server: Stopping server on .*",
"WARN org.apache.hadoop.hdfs.server.namenode.NameNode: Encountered exception during format:",
"WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of .* to reach .* \(unavailableStorages=\[.*\], storagePolicy=BlockStoragePolicy{HOT:.*, storageTypes=\[.*\], creationFallbacks=\[\], replicationFallbacks=\[.*\]}, newBlock=.*\) All required storage types are unavailable:  unavailableStorages=\[.*\], storagePolicy=BlockStoragePolicy{HOT:.*, storageTypes=\[.*\], creationFallbacks=\[\], replicationFallbacks=\[.*\]}",
"WARN org.apache.hadoop.hdfs.server.common.Storage: Storage directory .* does not exist",
"DEBUG PoolThreadCache: Freed .* thread\-local buffer\(s\) from thread: .*"
]
hadoopLogTemplate = templatebyProf +templatebyYJH + errored

if __name__ == "__main__":
    print(len(templatebyProf))
    print(len(templatebyYJH))
    print(len(hadoopLogTemplate))
